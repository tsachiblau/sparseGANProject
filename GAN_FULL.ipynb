{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_FULL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPj3NJDqNN4mI5lXm1e0y6/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsachiblau/sparseGANProject/blob/master/GAN_FULL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1WxwHJ6nVE_",
        "colab_type": "text"
      },
      "source": [
        "parameters setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2PNr1w4nSLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "ab2c0cae-f44a-41e3-e04e-6b41802b7b3f"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(10)\n",
        "# params\n",
        "noise_dim = 100\n",
        "batch_size = 16\n",
        "half_batch_size = int(batch_size/2)\n",
        "steps_per_epoch = 3750  # batch_size * steps_per_epoch = epoch size (train size)\n",
        "epochs = 10\n",
        "img_rows, img_cols, channels = 28, 28, 1\n",
        "sparse_dim = img_rows * img_cols * channels\n",
        "L1_weight = 10   # Loss = data_fidelity + L1_weight * L1_norm\n",
        "thr_value = 0.1 # x = |x| >= thr_value ? x : 0\n",
        "\n",
        "# optimizer - LR and Beta1 are hyper parameters\n",
        "optimizer = Adam(0.0002,0.5)\n",
        "optimizer2 = Adam(1e-6)\n",
        "train_only_fours = False  # overfit to 4 creation - sanity check\n",
        "warm_start_discriminator = False # init discriminator by training it *a little bit* as classifier\n",
        "load_dict = True  #load pretrained dictionary\n",
        "use_noisy_labels = False # use soft labels and not 0,1 to train\n",
        "use_reconstructor = False\n",
        "#\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Thu Mar 19 11:37:18 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.59       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLWpdwzKndYM",
        "colab_type": "text"
      },
      "source": [
        "import data set + preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ywLxYUlnfd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "52f26f97-a805-4d4f-d50b-b625a221af38"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = (x_train.astype(np.float32) - 127.5) / 127.5  # normalize between +1 -1\n",
        "\n",
        "# take only digit 4\n",
        "x_four_digit = x_train[y_train==4]\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "x_train = x_train.reshape(-1, img_rows*img_cols*channels) # each image as vector\n",
        "x_four_digit = x_four_digit.reshape(-1, img_rows*img_cols*channels) # each image as vector\n",
        "x_test = x_test.reshape(-1, img_rows*img_cols*channels) # each image as vector\n",
        "np.random.shuffle(x_train)\n",
        "np.random.shuffle(x_four_digit)\n",
        "print(x_train.shape)\n",
        "print(x_four_digit.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "(60000, 784)\n",
            "(5842, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IumGUSd-niqX",
        "colab_type": "text"
      },
      "source": [
        "Load dictionary from pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMs3TWBqnkwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "572f485e-3ab5-4ba6-9a7e-5245a8364dfc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import DictionaryLearning\n",
        "import pickle\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "#dictionary file name\n",
        "file_name = 'dictionary20.pkl'\n",
        "\n",
        "#check if dictionary exists\n",
        "if not path.exists(file_name):\n",
        "  \n",
        "  d=DictionaryLearning(n_components=sparse_dim, max_iter=10)\n",
        "  # train dictionary\n",
        "  d.fit(x_train[1:10000, :])\n",
        "  dictionary_loaded2 = np.transpose(d.components_)\n",
        "  print(dictionary.shape)\n",
        "\n",
        "  with open(file_name, 'wb') as output:\n",
        "    pickle.dump(d, output, pickle.HIGHEST_PROTOCOL)\n",
        "  print(\"created new dictionary\")\n",
        "\n",
        "else:\n",
        "  with open(file_name, 'rb') as input:\n",
        "    d = pickle.load(input)\n",
        "    dictionary_loaded2 = np.transpose(d.components_)\n",
        "  # print(d)\n",
        "  print(\"loaded dictionary\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded dictionary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DictionaryLearning from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4YjSQxpnm-I",
        "colab_type": "text"
      },
      "source": [
        "get transformed data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB2jDg-gnq7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import savetxt\n",
        "transformed_x = d.transform(x_train)\n",
        "transformed_x.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EAiUsENos_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "4296fee5-475e-423e-ab5b-cbb6e3f5a96c"
      },
      "source": [
        "savetxt('transformed_x.csv', transformed_x, delimiter=',')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-618fbaf99ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformed_x.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'savetxt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z0LUqUKnuDr",
        "colab_type": "text"
      },
      "source": [
        "Create NN modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqn6ISADnwQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with pursuit and thresholding approach\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.initializers import RandomNormal, glorot_uniform\n",
        "from keras.layers import Dense, Conv2D, Flatten, Reshape, Dropout,AveragePooling2D, MaxPooling2D, BatchNormalization, Lambda, ReLU, ThresholdedReLU, Activation, GaussianNoise\n",
        "from tensorflow.linalg import matmul, matrix_transpose\n",
        "from tensorflow.math import abs\n",
        "from keras import regularizers\n",
        "from keras.losses import kullback_leibler_divergence, binary_crossentropy\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "import tensorflow as tf\n",
        "from tensorflow import cast\n",
        "from keras.activations import sigmoid\n",
        "from keras.utils import np_utils\n",
        "from keras.backend import random_normal,shape,exp\n",
        "\n",
        "\n",
        "# costum loss - TODO:: parameter to TUNE\n",
        "def gaussian_prob_loss(y_actual,y_pred):\n",
        "  probs = 1 - K.square(K.exp(-0.5*K.square(y_pred)))    #1 - prob of y_pred to be drawn from gaussian with mu = 0, std = 1\n",
        "  return probs\n",
        "\n",
        "\n",
        "\n",
        "# would cause small entries in absulute values to be set to zero and will create sparse vectors\n",
        "def soft_threshlding(x):\n",
        "  above_thr_pos = x * cast(x >= thr_value, tf.float32)\n",
        "  below_thr_neg = x * cast(x <= -thr_value, tf.float32)\n",
        "  return (above_thr_pos + below_thr_neg)\n",
        "\n",
        "\n",
        "\n",
        "get_custom_objects().update({'soft_threshlding': Activation(soft_threshlding)})\n",
        "\n",
        "\n",
        "\n",
        "# classifier\n",
        "def create_trained_classifier():\n",
        "    classifier = Sequential()\n",
        "    #16 - better\n",
        "    classifier.add(Conv2D(16, (3, 3), strides=(2,2),padding='same', kernel_regularizer=regularizers.l2(0.01),kernel_initializer=RandomNormal(0, 0.02), input_shape=(img_cols, img_rows, channels),name=\"input\"))\n",
        "    #classifier.add(BatchNormalization(name=\"BN0\"))\n",
        "    classifier.add(LeakyReLU(0.2,name=\"Activation0\"))\n",
        "    #32 - better\n",
        "    classifier.add(Conv2D(32, (3, 3), strides=(2,2), padding='same',  kernel_regularizer=regularizers.l2(0.01),kernel_initializer=RandomNormal(0, 0.02),name=\"conv1\"))\n",
        "    #classifier.add(BatchNormalization(name=\"BN1\"))\n",
        "    classifier.add(LeakyReLU(0.2,name=\"Activation1\"))\n",
        "    # 64 - better\n",
        "    classifier.add(Conv2D(64, (3, 3), strides=(2,2), padding='same',  kernel_regularizer=regularizers.l2(0.01),kernel_initializer=RandomNormal(0, 0.02),name=\"conv2\"))\n",
        "    #classifier.add(BatchNormalization(name=\"BN2\"))\n",
        "    classifier.add(LeakyReLU(0.2,name=\"Activation2\"))\n",
        "    \n",
        "    classifier.add(Flatten(name=\"Flatten\"))\n",
        "    classifier.add(Dropout(0.4,name=\"dropout\"))\n",
        "    classifier.add(Dense(10, activation='softmax',kernel_regularizer=regularizers.l2(0.01),kernel_initializer=RandomNormal(0, 0.02),name=\"Dense\"))   # 1 number which indicates real / fake\n",
        "    classifier.trainable = True\n",
        "    classifier.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    model=classifier.fit(x_train.reshape(-1,img_rows,img_cols,channels), y_train, \n",
        "                              validation_data=(x_test.reshape(-1,img_rows,img_cols,channels), y_test), epochs=5, batch_size=64)\n",
        "    return classifier\n",
        "# input: image size\n",
        "# output: 1 number - 1 real 0 fake\n",
        "# descriminator:\n",
        "def create_discriminator():\n",
        "  if(warm_start_discriminator):\n",
        "    classifier = create_trained_classifier()\n",
        "    classifier.pop()\n",
        "    classifier.add(Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01),kernel_initializer=RandomNormal(0, 0.02),name=\"Dense\"))   # 1 number which indicates real / fake\n",
        "    classifier.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return classifier\n",
        "  else:\n",
        "    discriminator = Sequential()\n",
        "    #16 - better\n",
        "    discriminator.add(Conv2D(16, (3, 3), strides=(2,2),padding='same', kernel_regularizer=regularizers.l2(0.01),kernel_initializer=RandomNormal(0, 0.02), input_shape=(img_cols, img_rows, channels),name=\"input\"))\n",
        "    #discriminator.add(BatchNormalization(name=\"BN0\"))\n",
        "    discriminator.add(LeakyReLU(0.2,name=\"Activation0\"))\n",
        "    #32 - better\n",
        "    discriminator.add(Conv2D(32, (3, 3), strides=(2,2), padding='same',  kernel_regularizer=regularizers.l2(0.01),kernel_initializer=RandomNormal(0, 0.02),name=\"conv1\"))\n",
        "    #discriminator.add(BatchNormalization(name=\"BN1\"))\n",
        "    discriminator.add(LeakyReLU(0.2,name=\"Activation1\"))\n",
        "    # 64 - better\n",
        "    discriminator.add(Conv2D(64, (3, 3), strides=(2,2), padding='same',  kernel_regularizer=regularizers.l2(0.01),kernel_initializer=RandomNormal(0, 0.02),name=\"conv2\"))\n",
        "    #discriminator.add(BatchNormalization(name=\"BN2\"))\n",
        "    discriminator.add(LeakyReLU(0.2,name=\"Activation2\"))\n",
        "    \n",
        "    discriminator.add(Flatten(name=\"Flatten\"))\n",
        "    discriminator.add(Dropout(0.4,name=\"dropout\"))\n",
        "    discriminator.add(Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01),kernel_initializer=RandomNormal(0, 0.02),name=\"Dense\"))   # 1 number which indicates real / fake\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return discriminator\n",
        "\n",
        "\n",
        "# input: sparse vec size\n",
        "# output: noise size\n",
        "def create_encoder():\n",
        "    reconstructor = Sequential()\n",
        "    init = RandomNormal(0,stddev=0.02)\n",
        "\n",
        "    reconstructor.add(Dense(512,  kernel_initializer=init, input_dim=(img_rows*img_cols*channels),name=\"rec_input\"))\n",
        "    reconstructor.add(BatchNormalization(name=\"BN0\"))\n",
        "    reconstructor.add(LeakyReLU(0.2,name=\"activation0\"))\n",
        "\n",
        "    reconstructor.add(Dense(256, kernel_initializer=init,name=\"hidden1\"))\n",
        "    reconstructor.add(BatchNormalization(name=\"BN1\"))\n",
        "    reconstructor.add(LeakyReLU(0.2,name=\"activation1\"))\n",
        "\n",
        "    reconstructor.add(Dense(noise_dim, kernel_initializer=init,name=\"hidden2\"))\n",
        "    reconstructor.add(BatchNormalization(name=\"BN2\"))\n",
        "    return reconstructor\n",
        "\n",
        "# input: noise size\n",
        "# output: sparse vec size\n",
        "def create_generator():\n",
        "    generator = Sequential()\n",
        "    init = RandomNormal(0,stddev=0.02)\n",
        "    generator.add(Dense(256, kernel_initializer=init, input_dim=noise_dim,name=\"init\"))\n",
        "    generator.add(BatchNormalization(name=\"BN1\"))\n",
        "    generator.add(LeakyReLU(0.2,name=\"Activation1\"))\n",
        "\n",
        "    generator.add(Dense(512,kernel_initializer=init,name=\"hidden1\"))\n",
        "    generator.add(BatchNormalization(name=\"BN2\"))\n",
        "    generator.add(LeakyReLU(0.2,name=\"Activation2\"))\n",
        "\n",
        "    # generator.add(Dense(1024,kernel_initializer=init,name=\"hidden2\"))\n",
        "    # generator.add(BatchNormalization(name=\"BN3\"))\n",
        "    # generator.add(LeakyReLU(0.2,name=\"Activation3\"))\n",
        "\n",
        "    generator.add(Dense(sparse_dim, kernel_initializer=init,name=\"hidden3\"))\n",
        "    # soft thresholding activation\n",
        "    generator.add(BatchNormalization(name=\"BN4\"))\n",
        "    generator.add(Activation(soft_threshlding, name='soft_threshlding'))\n",
        "    return generator\n",
        "\n",
        "\n",
        "generator = create_generator()\n",
        "encoder = create_encoder()\n",
        "discriminator = create_discriminator()\n",
        "# naming:\n",
        "generator.name = \"Gen\"\n",
        "encoder.name = \"Rec\"\n",
        "discriminator.name = \"Disc\"\n",
        "#\n",
        "discriminator.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aifg-d0Fnxmd",
        "colab_type": "text"
      },
      "source": [
        "create GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_iLhqi3n3cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_gan(discriminator, generator):\n",
        "    discriminator.trainable=False\n",
        "    gan_input = Input(shape=(noise_dim,))\n",
        "    # G(z) is a sparse vector\n",
        "    sparse_vec = generator(gan_input)   # sparse_dim size - it is thresholded\n",
        "    # a layer that creates the multiplication between the sparse tensor with dictionary to create image\n",
        "    fake_image_vec = Lambda(lambda x: matrix_transpose(matmul(dictionary_loaded2,matrix_transpose(x))),name='mul_in_D')(sparse_vec)\n",
        "    # reshape to image size - will be inserted to discriminator\n",
        "    fake_image = Reshape((img_rows, img_cols, channels),name=\"Reshape_as_im\")(fake_image_vec)\n",
        "    gan_output = discriminator(fake_image)  # the descriminator output\n",
        "    # todo - double output double loss\n",
        "    gan = Model(gan_input, [gan_output,sparse_vec])\n",
        "    gan.compile(loss=['binary_crossentropy','mean_absolute_error'], optimizer=optimizer,loss_weights=[1,L1_weight])\n",
        "    plot_model(gan, to_file='model_plot.png', show_shapes=True, show_layer_names=True,expand_nested=True)\n",
        "    return gan\n",
        "\n",
        "gan = create_gan(discriminator,generator)\n",
        "print(gan.summary())\n",
        "\n",
        "print(discriminator.summary())\n",
        "print(discriminator.layers[0].trainable)\n",
        "print(gan.get_layer(name=\"Disc\").trainable)\n",
        "print(gan.get_layer(name=\"Gen\").trainable)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3oFcp7fn4Rz",
        "colab_type": "text"
      },
      "source": [
        "create AE module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6DN1-F0n688",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define ae\n",
        "vae_input = Input(shape=(sparse_dim,))\n",
        "encoder_output = encoder(vae_input)\n",
        "# G(z) is a sparse vector\n",
        "sparse_vec2 = generator(encoder_output)   # sparse_dim size - it is thresholded\n",
        "# a layer that creates the multiplication between the sparse tensor with dictionary to create image\n",
        "fake_image_vec2 = Lambda(lambda x: matrix_transpose(matmul(dictionary_loaded2,matrix_transpose(x))),name='mul_in_D')(sparse_vec2)\n",
        "\n",
        "vae = Model(vae_input, [encoder_output, sparse_vec2,fake_image_vec2])  #todo add L2 on image\n",
        "vae.compile(loss=[gaussian_prob_loss,'mean_absolute_error','mean_squared_error'],loss_weights=[0.1,1,1],optimizer=optimizer)\n",
        "plot_model(vae, to_file='model_plot2.png', show_shapes=True, show_layer_names=True,expand_nested=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNAd0KUvn-bP",
        "colab_type": "text"
      },
      "source": [
        "Train AE sanity check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KwweyHvoEMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sanity check\n",
        "from numpy.random import uniform\n",
        "import time\n",
        "\n",
        "t = time.time()\n",
        "zeros = np.zeros(60000*100)\n",
        "zeros = zeros.reshape(60000,100)\n",
        "vae.fit(transformed_x,[zeros,transformed_x,x_train],epochs = 20, shuffle=True)\n",
        "print(\"elapsed \",time.time()-t)\n",
        "def show_images2():\n",
        "    noise = np.random.normal(0, 1, size=(100, 100))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = np.matmul(np.transpose(d.components_),np.transpose(generated_images))\n",
        "    generated_images = np.transpose(generated_images)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, image in enumerate(generated_images):\n",
        "        plt.subplot(10, 10, i+1)\n",
        "        if channels == 1:\n",
        "            plt.imshow(image.reshape((img_rows, img_cols)), cmap='gray')\n",
        "        else:\n",
        "            plt.imshow(image.reshape((img_rows, img_cols, channels)))\n",
        "        plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "idx = int(uniform(0,59999))\n",
        "\n",
        "im = np.array(x_train[idx,:])\n",
        "im = im.reshape(28,28)\n",
        "plt.imshow(im,cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "vae_pred = vae.predict(transformed_x[idx:idx+1,:])[2]\n",
        "im_r = vae_pred[0].reshape(28,28)\n",
        "plt.imshow(im_r,cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# gen new data\n",
        "\n",
        "show_images2()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTuwCnoHoHzm",
        "colab_type": "text"
      },
      "source": [
        "train GAN sanity check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6ukYdtFoLH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import uniform\n",
        "\n",
        "def d_ratio_on_fake(x):\n",
        "  return np.sum(discriminator.predict(x))/x.shape[0]\n",
        "\n",
        "def generate_fake_samples(size):\n",
        "  noise = np.random.normal(0, 1, size=(size, noise_dim))  # will generate batch amount of noise vectors (gaussian distribution)    \n",
        "  fake_x = generator.predict(noise) # create G(z)\n",
        "  # fake is (size,sparse_dim)\n",
        "  fake_x = np.transpose(fake_x)\n",
        "  # multiply by dict\n",
        "  fake_x = np.matmul(dictionary_loaded2,fake_x)\n",
        "  fake_x = np.transpose(fake_x)\n",
        "  fake_x = fake_x.reshape(size,img_rows,img_cols,channels)\n",
        "  return fake_x\n",
        "\n",
        "def generate_real_samples(size):\n",
        "  real_idx = np.random.randint(0, x_train.shape[0], size=half_batch_size)\n",
        "  real_x = x_train[real_idx] # sample real image\n",
        "  real_x = real_x.reshape(fake_x.shape)\n",
        "  return real_x\n",
        "\n",
        "def plot_statistics(descriminator_real_probability, descriminator_fake_probability, descriminator_real_loss, descriminator_fake_loss):\n",
        "  plt.figure()\n",
        "  plt.subplot(131)\n",
        "  plt.title('discriminator probability')\n",
        "  plt.plot(iters, descriminator_real_probability)\n",
        "  plt.plot(iters, descriminator_fake_probability)\n",
        "  plt.legend(['real', 'fake'])\n",
        "\n",
        "  plt.subplot(132)\n",
        "  plt.title('discriminator loss')\n",
        "  plt.plot(iters, descriminator_real_loss)\n",
        "  plt.plot(iters, descriminator_fake_loss)\n",
        "  plt.legend(['real', 'fake'])\n",
        "  plt.show()\n",
        "\n",
        "def show_images(noise):\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = np.matmul(dictionary_loaded2,np.transpose(generated_images))\n",
        "    generated_images = np.transpose(generated_images)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, image in enumerate(generated_images):\n",
        "        plt.subplot(10, 10, i+1)\n",
        "        if channels == 1:\n",
        "            plt.imshow(image.reshape((img_rows, img_cols)), cmap='gray')\n",
        "        else:\n",
        "            plt.imshow(image.reshape((img_rows, img_cols, channels)))\n",
        "        plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def label_smoothing(y):  #recieves arrray of ones / zeros and smooth it randomly\n",
        "  return (y + (random(y.shape) * 0.3))\n",
        "\n",
        "stop = False\n",
        "fake_x = 0\n",
        "\n",
        "descriminator_real_probability = np.array([])\n",
        "descriminator_fake_probability = np.array([])\n",
        "\n",
        "descriminator_fake_loss = np.array([])\n",
        "descriminator_real_loss = np.array([])\n",
        "\n",
        "reconstructor_loss_list = np.array([])\n",
        "\n",
        "iters= np.array([])\n",
        "n_d_real_list = np.array([])\n",
        "n_d_fake_list = np.array([])\n",
        "\n",
        "d_loss_real = 0 \n",
        "d_loss_fake = 0\n",
        "g_loss = 0\n",
        "\n",
        "n_d = 10 # 1 10 100\n",
        "n_g = 10  # 1 10 100\n",
        "n_r = 1  # 1 10 100\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):  #epochs\n",
        "  for iter in range(int(steps_per_epoch)):\n",
        "    i = iter + int(epoch*(steps_per_epoch))\n",
        "\n",
        "    #discriminator.trainable = True\n",
        "    # train D_fake\n",
        "    for batch in range(n_d):  #steps_per_epoch\n",
        "        fake_x = generate_fake_samples(half_batch_size)\n",
        "        # fake_x is of size (half_batch_size,rows,cols,channels)\n",
        "        disc_y_fake = uniform(0,0.2,half_batch_size)\n",
        "        # train_on_batch recieves data and desired output\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_x, disc_y_fake)\n",
        "        real_x =generate_real_samples(half_batch_size)\n",
        "        disc_y_real = uniform(0.7,1.2,half_batch_size)\n",
        "        # train_on_batch recieves data and desired output\n",
        "        d_loss_real = discriminator.train_on_batch(real_x, disc_y_real)\n",
        "\n",
        "    #discriminator.trainable = False\n",
        "    # train G\n",
        "    for batch in range(n_g):  #steps_per_epoch\n",
        "        noise = np.random.normal(0, 1, size=(batch_size, noise_dim))  # will generate batch amount of noise vectors (gaussian distribution)\n",
        "        # train GAN (descriminator weights are fixed)\n",
        "        y_gen_1 = uniform(0.7,1,batch_size)\n",
        "        y_gen_2 = np.zeros(batch_size * sparse_dim)\n",
        "        y_gen_2_reshape = y_gen_2.reshape(batch_size,sparse_dim)\n",
        "        g_loss = gan.train_on_batch(noise, [y_gen_1, y_gen_2_reshape])\n",
        "\n",
        "    ########################################################## statistics ########################################################## \n",
        "    descriminator_fake_loss = np.append(descriminator_fake_loss ,d_loss_fake)        \n",
        "    descriminator_real_loss = np.append(descriminator_real_loss ,d_loss_real)        \n",
        "\n",
        "    real_data = generate_real_samples(batch_size)\n",
        "    fake_data = generate_fake_samples(batch_size)\n",
        "    descriminator_fake_probability = np.append(descriminator_fake_probability ,d_ratio_on_fake(fake_data))        \n",
        "    descriminator_real_probability = np.append(descriminator_real_probability ,d_ratio_on_fake(real_data))        \n",
        "    iters =  np.append(iters,i)\n",
        "    if(i%10==0):\n",
        "      plot_statistics(descriminator_real_probability, descriminator_fake_probability, descriminator_real_loss, descriminator_fake_loss)\n",
        "      noise = np.random.normal(0, 1, size=(10, noise_dim))\n",
        "      show_images(noise)\n",
        "\n",
        "\n",
        "\n",
        "  print(f'Epoch: {epoch + 1} \\t Discriminator Loss_fake: {d_loss_fake} \\t Discriminator Loss_real: {d_loss_real}\\t\\t Generator Loss: {g_loss}')\n",
        "  plot_statistics(descriminator_real_probability, descriminator_fake_probability, descriminator_real_loss, descriminator_fake_loss)\n",
        "\n",
        "  noise = np.random.normal(0, 1, size=(100, noise_dim))\n",
        "  show_images(noise)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df7TgLe8ozJw",
        "colab_type": "text"
      },
      "source": [
        "Combined training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_6YyGtXpBBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import uniform\n",
        "\n",
        "def d_ratio_on_samples(x):\n",
        "  return np.sum(discriminator.predict(x))/x.shape[0]\n",
        "\n",
        "def generate_fake_samples(size):\n",
        "  noise = np.random.normal(0, 1, size=(size, noise_dim))  # will generate batch amount of noise vectors (gaussian distribution)    \n",
        "  fake_x = generator.predict(noise) # create G(z)\n",
        "  # fake is (size,sparse_dim)\n",
        "  fake_x = np.transpose(fake_x)\n",
        "  # multiply by dict\n",
        "  fake_x = np.matmul(dictionary_loaded2,fake_x)\n",
        "  fake_x = np.transpose(fake_x)\n",
        "  fake_x = fake_x.reshape(size,img_rows,img_cols,channels)\n",
        "  return fake_x\n",
        "\n",
        "def generate_real_samples(size):\n",
        "  real_idx = np.random.randint(0, x_train.shape[0], size=half_batch_size)\n",
        "  real_x = x_train[real_idx] # sample real image\n",
        "  real_x = real_x.reshape(fake_x.shape)\n",
        "  return real_x\n",
        "\n",
        "def plot_statistics(descriminator_real_probability, descriminator_fake_probability):\n",
        "  plt.figure()\n",
        "  plt.title('discriminator probability')\n",
        "  plt.plot(iters, descriminator_real_probability)\n",
        "  plt.plot(iters, descriminator_fake_probability)\n",
        "  plt.legend(['real', 'fake'])\n",
        "  plt.show()\n",
        "\n",
        "def show_images(noise):\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = np.matmul(dictionary_loaded2,np.transpose(generated_images))\n",
        "    generated_images = np.transpose(generated_images)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, image in enumerate(generated_images):\n",
        "        plt.subplot(10, 10, i+1)\n",
        "        if channels == 1:\n",
        "            plt.imshow(image.reshape((img_rows, img_cols)), cmap='gray')\n",
        "        else:\n",
        "            plt.imshow(image.reshape((img_rows, img_cols, channels)))\n",
        "        plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "descriminator_real_probability = np.array([])\n",
        "descriminator_fake_probability = np.array([])\n",
        "iters= np.array([])\n",
        "\n",
        "# training amount params\n",
        "n_d = 30 # 1 10 100\n",
        "n_g = 10  # 1 10 100\n",
        "n_r = 10  # 1 10 100\n",
        "i = 0\n",
        "\n",
        "for epoch in range(epochs):  #epochs\n",
        "  for iter in range(int(steps_per_epoch)):\n",
        "    i += 1\n",
        "    # train D\n",
        "    for batch in range(n_d):  #steps_per_epoch\n",
        "        fake_x = generate_fake_samples(half_batch_size)\n",
        "        # fake_x is of size (half_batch_size,rows,cols,channels)\n",
        "        disc_y_fake = uniform(0,0.2,half_batch_size)\n",
        "        # train_on_batch recieves data and desired output\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_x, disc_y_fake)\n",
        "        real_x =generate_real_samples(half_batch_size)\n",
        "        disc_y_real = uniform(0.7,1.2,half_batch_size)\n",
        "        # train_on_batch recieves data and desired output\n",
        "        d_loss_real = discriminator.train_on_batch(real_x, disc_y_real)\n",
        "\n",
        "    # train G\n",
        "    for batch in range(n_g):  #steps_per_epoch\n",
        "        noise = np.random.normal(0, 1, size=(batch_size, noise_dim))  # will generate batch amount of noise vectors (gaussian distribution)\n",
        "        # train GAN (descriminator weights are fixed)\n",
        "        y_gen_1 = uniform(0.7,1,batch_size)\n",
        "        y_gen_2 = np.zeros(batch_size * sparse_dim)\n",
        "        y_gen_2_reshape = y_gen_2.reshape(batch_size,sparse_dim)\n",
        "        g_loss = gan.train_on_batch(noise, [y_gen_1, y_gen_2_reshape])\n",
        "\n",
        "    # train R\n",
        "    for batch in range(n_r):  #steps_per_epoch\n",
        "      rand_idxs = np.random.randint(0, x_train.shape[0], size=64)\n",
        "      x = x_train[rand_idxs] # sample real transformed image\n",
        "      trans_x = transformed_x[rand_idxs]\n",
        "      zeros = np.zeros(64*noise_dim)\n",
        "      zeros = zeros.reshape(64,noise_dim)\n",
        "      labels = [zeros,trans_x,x]\n",
        "      r_loss = vae.train_on_batch(trans_x,labels)\n",
        "\n",
        "    ########################################################## statistics ##########################################################       \n",
        "\n",
        "    real_data = generate_real_samples(batch_size)\n",
        "    fake_data = generate_fake_samples(batch_size)\n",
        "    descriminator_fake_probability = np.append(descriminator_fake_probability ,d_ratio_on_samples(fake_data))        \n",
        "    descriminator_real_probability = np.append(descriminator_real_probability ,d_ratio_on_samples(real_data))        \n",
        "    iters =  np.append(iters,i)\n",
        "    if(i%100==0):\n",
        "      plot_statistics(descriminator_real_probability, descriminator_fake_probability)\n",
        "      noise = np.random.normal(0, 1, size=(10, noise_dim))\n",
        "      show_images(noise)\n",
        "\n",
        "\n",
        "\n",
        "  print(f'Epoch: {epoch + 1} \\t Discriminator Loss_fake: {d_loss_fake} \\t Discriminator Loss_real: {d_loss_real}\\t\\t Generator Loss: {g_loss}')\n",
        "  plot_statistics(descriminator_real_probability, descriminator_fake_probability, descriminator_real_loss, descriminator_fake_loss)\n",
        "\n",
        "  noise = np.random.normal(0, 1, size=(100, noise_dim))\n",
        "  show_images(noise)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}